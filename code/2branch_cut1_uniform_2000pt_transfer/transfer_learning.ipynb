{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_cut1.drop(morpho_data_cut1.columns[3:23], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_cut1.drop(['source_x',\"cuttype\",\"dataset\"], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_dome.drop(['source_x',\"cuttype\",\"dataset\"], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_cut1.drop(morpho_data_cut1.columns[3:23], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_cut1.drop(['source_x',\"cuttype\",\"dataset\"], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_dome.drop(['source_x',\"cuttype\",\"dataset\"], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_cut1.drop(morpho_data_cut1.columns[3:23], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_cut1.drop(['source_x',\"cuttype\",\"dataset\"], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_dome.drop(['source_x',\"cuttype\",\"dataset\"], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_cut1.drop(morpho_data_cut1.columns[3:23], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_cut1.drop(['source_x',\"cuttype\",\"dataset\"], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_dome.drop(['source_x',\"cuttype\",\"dataset\"], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_cut1.drop(morpho_data_cut1.columns[3:23], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_cut1.drop(['source_x',\"cuttype\",\"dataset\"], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_dome.drop(['source_x',\"cuttype\",\"dataset\"], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_cut1.drop(morpho_data_cut1.columns[3:23], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_cut1.drop(['source_x',\"cuttype\",\"dataset\"], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_dome.drop(['source_x',\"cuttype\",\"dataset\"], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_cut1.drop(morpho_data_cut1.columns[3:23], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_cut1.drop(['source_x',\"cuttype\",\"dataset\"], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_dome.drop(['source_x',\"cuttype\",\"dataset\"], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_cut1.drop(morpho_data_cut1.columns[3:23], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_cut1.drop(['source_x',\"cuttype\",\"dataset\"], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_dome.drop(['source_x',\"cuttype\",\"dataset\"], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_cut1.drop(morpho_data_cut1.columns[3:23], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_cut1.drop(['source_x',\"cuttype\",\"dataset\"], axis=1, inplace=True)\n",
      "d:\\Universityofleeds\\MSC_PROG\\Msc_project\\code\\pointnetfunct\\data_process_ml.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  morpho_data_dome.drop(['source_x',\"cuttype\",\"dataset\"], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import vtk\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import math, random\n",
    "\n",
    "random.seed = 42\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import scipy.spatial.distance\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "device = \"cuda\"\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import pointnetfunct.functions as fun\n",
    "import pointnetfunct.data_process_ml as data_process_ml\n",
    "\n",
    "from pointnetfunct.PointNet_dataset import Aneux_Dataset_load\n",
    "from pointnetfunct.PointNet_trainingfunct import run_model_3multi_head_dnn\n",
    "from pointnetfunct.PointNet_trainingfunct import run_model_get\n",
    "from pointnetfunct.PointNet_trainingfunct import load_model\n",
    "from pointnetfunct.PointNet_struct import Tnet,Transform,PointNet,PointNet_2Multihead,PointNet_3Multihead,PointNet_3Multihead_withDNN,PointNet_2Multimodal_withDNN\n",
    "from pointnetfunct.evaluation import show_cm_dl,show_graph,print_values,draw_rocgraph\n",
    "from pointnetfunct.test_functions import pointnet_2branch_testcase,pointnet_3_branch_testcase,pointnet_testcase\n",
    "\n",
    "#load datasets\n",
    "\n",
    "morpho_path = \".\\AneuX\\data-v1.0\\data\\morpho-per-cut.csv\"\n",
    "patient_path = \".\\AneuX\\data-v1.0\\data\\clinical.csv\"\n",
    "morpho_data_patient = data_process_ml.read_and_combine_data(morpho_path,patient_path)\n",
    "merged_dataset = data_process_ml.encode_column(morpho_data_patient)\n",
    "merged_dataset = data_process_ml.drop_columns(merged_dataset)\n",
    "morpho_data_cut1,morpho_data_dome = data_process_ml.output_cut1anddome(merged_dataset)\n",
    "#load Aneux Dataset\n",
    "Aneux_1000pt_Dataset_uniform = Aneux_Dataset_load('./Datasets/Aneux_Dataset_1000pt_sample_600train.pt')\n",
    "Aneux_1000pt_Dataset_uniform_test = Aneux_Dataset_load('./Datasets/Aneux_Dataset_1000pt_sample_100test.pt')\n",
    "\n",
    "Aneux_2000pt_Dataset_uniform = Aneux_Dataset_load('./Datasets/Aneux_Dataset_2000pt_sample_600train.pt')\n",
    "Aneux_2000pt_Dataset_uniform_test = Aneux_Dataset_load('./Datasets/Aneux_Dataset_2000pt_sample_100test.pt')\n",
    "\n",
    "Aneux_1000pt_Dataset_ppd = Aneux_Dataset_load('./Datasets/Aneux_Dataset_1000pt_ppd_600train.pt')\n",
    "Aneux_1000pt_Dataset_ppd_test = Aneux_Dataset_load('./Datasets/Aneux_Dataset_1000pt_ppd_100test.pt')\n",
    "\n",
    "Aneux_2000pt_Dataset_ppd = Aneux_Dataset_load('./Datasets/Aneux_Dataset_2000pt_ppd_600train.pt')\n",
    "Aneux_2000pt_Dataset_ppd_test = Aneux_Dataset_load('./Datasets/Aneux_Dataset_2000pt_ppd_100test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = Aneux_2000pt_Dataset_uniform_test\n",
    "Dataset.cuttype = \"cut1\"\n",
    "train_size = int(len(Dataset) * 0.8) # 50% training data\n",
    "valid_size = len(Dataset) - train_size\n",
    "train_data, valid_data = random_split(Dataset, [train_size, valid_size])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=20,\n",
    "    shuffle=True,\n",
    "    #num_workers=2, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_data,\n",
    "    batch_size=20, # Forward pass only so batch size can be larger\n",
    "    shuffle=False,\n",
    "    #num_workers=2, \n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 training loss:  0.800 training accuracy:  48.8%  test loss:  0.716 test accuracy:  50.0%\n",
      "epoch: 1 training loss:  0.703 training accuracy:  48.8%  test loss:  0.689 test accuracy:  55.0%\n",
      "epoch: 2 training loss:  0.732 training accuracy:  65.0%  test loss:  0.824 test accuracy:  45.0%\n",
      "epoch: 3 training loss:  0.618 training accuracy:  67.5%  test loss:  0.738 test accuracy:  50.0%\n",
      "epoch: 4 training loss:  0.565 training accuracy:  65.0%  test loss:  0.825 test accuracy:  40.0%\n",
      "epoch: 5 training loss:  0.522 training accuracy:  58.8%  test loss:  0.738 test accuracy:  50.0%\n",
      "epoch: 6 training loss:  0.541 training accuracy:  56.2%  test loss:  0.763 test accuracy:  50.0%\n",
      "epoch: 7 training loss:  0.542 training accuracy:  60.0%  test loss:  0.781 test accuracy:  50.0%\n",
      "epoch: 8 training loss:  0.522 training accuracy:  50.0%  test loss:  0.823 test accuracy:  55.0%\n",
      "epoch: 9 training loss:  0.539 training accuracy:  57.5%  test loss:  0.859 test accuracy:  55.0%\n",
      "epoch: 10 training loss:  0.463 training accuracy:  52.5%  test loss:  0.813 test accuracy:  55.0%\n",
      "epoch: 11 training loss:  0.435 training accuracy:  60.0%  test loss:  0.895 test accuracy:  60.0%\n",
      "epoch: 12 training loss:  0.390 training accuracy:  60.0%  test loss:  0.967 test accuracy:  50.0%\n",
      "epoch: 13 training loss:  0.444 training accuracy:  55.0%  test loss:  0.974 test accuracy:  50.0%\n",
      "epoch: 14 training loss:  0.332 training accuracy:  53.8%  test loss:  0.931 test accuracy:  60.0%\n",
      "epoch: 15 training loss:  0.353 training accuracy:  57.5%  test loss:  0.940 test accuracy:  55.0%\n",
      "epoch: 16 training loss:  0.410 training accuracy:  61.3%  test loss:  0.969 test accuracy:  55.0%\n",
      "epoch: 17 training loss:  0.409 training accuracy:  51.2%  test loss:  1.044 test accuracy:  55.0%\n",
      "epoch: 18 training loss:  0.443 training accuracy:  56.2%  test loss:  0.985 test accuracy:  60.0%\n",
      "epoch: 19 training loss:  0.421 training accuracy:  56.2%  test loss:  1.040 test accuracy:  55.0%\n",
      "epoch: 20 training loss:  0.516 training accuracy:  53.8%  test loss:  0.941 test accuracy:  55.0%\n",
      "epoch: 21 training loss:  0.393 training accuracy:  58.8%  test loss:  0.997 test accuracy:  55.0%\n",
      "epoch: 22 training loss:  0.369 training accuracy:  60.0%  test loss:  1.014 test accuracy:  55.0%\n",
      "epoch: 23 training loss:  0.353 training accuracy:  51.2%  test loss:  0.952 test accuracy:  55.0%\n",
      "epoch: 24 training loss:  0.364 training accuracy:  60.0%  test loss:  0.994 test accuracy:  55.0%\n",
      "epoch: 25 training loss:  0.312 training accuracy:  53.8%  test loss:  0.997 test accuracy:  60.0%\n",
      "epoch: 26 training loss:  0.297 training accuracy:  57.5%  test loss:  1.038 test accuracy:  50.0%\n",
      "epoch: 27 training loss:  0.332 training accuracy:  60.0%  test loss:  1.102 test accuracy:  45.0%\n",
      "epoch: 28 training loss:  0.353 training accuracy:  60.0%  test loss:  1.108 test accuracy:  50.0%\n",
      "epoch: 29 training loss:  0.230 training accuracy:  57.5%  test loss:  1.144 test accuracy:  65.0%\n",
      "epoch: 30 training loss:  0.344 training accuracy:  53.8%  test loss:  1.282 test accuracy:  60.0%\n",
      "epoch: 31 training loss:  0.321 training accuracy:  60.0%  test loss:  1.222 test accuracy:  60.0%\n",
      "epoch: 32 training loss:  0.300 training accuracy:  50.0%  test loss:  1.334 test accuracy:  50.0%\n",
      "epoch: 33 training loss:  0.363 training accuracy:  52.5%  test loss:  1.172 test accuracy:  55.0%\n",
      "epoch: 34 training loss:  0.225 training accuracy:  63.7%  test loss:  1.179 test accuracy:  60.0%\n",
      "epoch: 35 training loss:  0.451 training accuracy:  57.5%  test loss:  1.242 test accuracy:  65.0%\n",
      "epoch: 36 training loss:  0.249 training accuracy:  56.2%  test loss:  1.341 test accuracy:  55.0%\n",
      "epoch: 37 training loss:  0.329 training accuracy:  56.2%  test loss:  1.243 test accuracy:  55.0%\n",
      "epoch: 38 training loss:  0.367 training accuracy:  56.2%  test loss:  1.163 test accuracy:  60.0%\n",
      "epoch: 39 training loss:  0.239 training accuracy:  58.8%  test loss:  1.224 test accuracy:  60.0%\n",
      "epoch: 40 training loss:  0.292 training accuracy:  62.5%  test loss:  1.189 test accuracy:  60.0%\n",
      "epoch: 41 training loss:  0.325 training accuracy:  52.5%  test loss:  1.194 test accuracy:  60.0%\n",
      "epoch: 42 training loss:  0.360 training accuracy:  62.5%  test loss:  1.322 test accuracy:  50.0%\n",
      "epoch: 43 training loss:  0.306 training accuracy:  55.0%  test loss:  1.364 test accuracy:  50.0%\n",
      "epoch: 44 training loss:  0.297 training accuracy:  61.3%  test loss:  1.272 test accuracy:  55.0%\n",
      "epoch: 45 training loss:  0.301 training accuracy:  61.3%  test loss:  1.440 test accuracy:  45.0%\n",
      "epoch: 46 training loss:  0.255 training accuracy:  57.5%  test loss:  1.566 test accuracy:  45.0%\n",
      "epoch: 47 training loss:  0.226 training accuracy:  62.5%  test loss:  1.357 test accuracy:  45.0%\n",
      "epoch: 48 training loss:  0.278 training accuracy:  56.2%  test loss:  1.152 test accuracy:  60.0%\n",
      "epoch: 49 training loss:  0.348 training accuracy:  58.8%  test loss:  1.350 test accuracy:  60.0%\n",
      "epoch: 50 training loss:  0.164 training accuracy:  58.8%  test loss:  1.374 test accuracy:  60.0%\n",
      "epoch: 51 training loss:  0.145 training accuracy:  56.2%  test loss:  1.352 test accuracy:  60.0%\n",
      "epoch: 52 training loss:  0.368 training accuracy:  51.2%  test loss:  1.348 test accuracy:  50.0%\n",
      "epoch: 53 training loss:  0.233 training accuracy:  53.8%  test loss:  1.468 test accuracy:  45.0%\n",
      "epoch: 54 training loss:  0.524 training accuracy:  56.2%  test loss:  1.356 test accuracy:  50.0%\n",
      "epoch: 55 training loss:  0.287 training accuracy:  55.0%  test loss:  1.300 test accuracy:  45.0%\n",
      "epoch: 56 training loss:  0.386 training accuracy:  58.8%  test loss:  1.302 test accuracy:  40.0%\n",
      "epoch: 57 training loss:  0.338 training accuracy:  52.5%  test loss:  1.124 test accuracy:  50.0%\n",
      "epoch: 58 training loss:  0.244 training accuracy:  57.5%  test loss:  1.108 test accuracy:  45.0%\n",
      "epoch: 59 training loss:  0.233 training accuracy:  56.2%  test loss:  1.084 test accuracy:  55.0%\n",
      "epoch: 60 training loss:  0.207 training accuracy:  53.8%  test loss:  1.053 test accuracy:  60.0%\n",
      "epoch: 61 training loss:  0.307 training accuracy:  56.2%  test loss:  1.154 test accuracy:  55.0%\n",
      "epoch: 62 training loss:  0.297 training accuracy:  55.0%  test loss:  1.276 test accuracy:  45.0%\n",
      "epoch: 63 training loss:  0.371 training accuracy:  52.5%  test loss:  1.204 test accuracy:  50.0%\n",
      "epoch: 64 training loss:  0.354 training accuracy:  48.8%  test loss:  1.215 test accuracy:  50.0%\n",
      "epoch: 65 training loss:  0.264 training accuracy:  53.8%  test loss:  1.069 test accuracy:  55.0%\n",
      "epoch: 66 training loss:  0.262 training accuracy:  58.8%  test loss:  1.025 test accuracy:  60.0%\n",
      "epoch: 67 training loss:  0.252 training accuracy:  52.5%  test loss:  1.057 test accuracy:  60.0%\n",
      "epoch: 68 training loss:  0.416 training accuracy:  50.0%  test loss:  1.138 test accuracy:  55.0%\n",
      "epoch: 69 training loss:  0.260 training accuracy:  52.5%  test loss:  1.143 test accuracy:  50.0%\n",
      "epoch: 70 training loss:  0.225 training accuracy:  48.8%  test loss:  1.200 test accuracy:  60.0%\n",
      "epoch: 71 training loss:  0.231 training accuracy:  56.2%  test loss:  1.125 test accuracy:  55.0%\n",
      "epoch: 72 training loss:  0.294 training accuracy:  53.8%  test loss:  1.237 test accuracy:  50.0%\n",
      "epoch: 73 training loss:  0.269 training accuracy:  52.5%  test loss:  1.350 test accuracy:  50.0%\n",
      "epoch: 74 training loss:  0.177 training accuracy:  57.5%  test loss:  1.427 test accuracy:  50.0%\n",
      "epoch: 75 training loss:  0.257 training accuracy:  58.8%  test loss:  1.400 test accuracy:  40.0%\n",
      "epoch: 76 training loss:  0.210 training accuracy:  52.5%  test loss:  1.438 test accuracy:  50.0%\n",
      "epoch: 77 training loss:  0.234 training accuracy:  51.2%  test loss:  1.472 test accuracy:  45.0%\n",
      "epoch: 78 training loss:  0.177 training accuracy:  52.5%  test loss:  1.502 test accuracy:  45.0%\n",
      "epoch: 79 training loss:  0.190 training accuracy:  46.2%  test loss:  1.525 test accuracy:  40.0%\n",
      "epoch: 80 training loss:  0.136 training accuracy:  46.2%  test loss:  1.657 test accuracy:  45.0%\n",
      "epoch: 81 training loss:  0.147 training accuracy:  58.8%  test loss:  1.838 test accuracy:  35.0%\n",
      "epoch: 82 training loss:  0.192 training accuracy:  50.0%  test loss:  1.805 test accuracy:  35.0%\n",
      "epoch: 83 training loss:  0.231 training accuracy:  47.5%  test loss:  1.733 test accuracy:  40.0%\n",
      "epoch: 84 training loss:  0.130 training accuracy:  52.5%  test loss:  1.444 test accuracy:  45.0%\n",
      "epoch: 85 training loss:  0.170 training accuracy:  51.2%  test loss:  1.421 test accuracy:  55.0%\n",
      "epoch: 86 training loss:  0.124 training accuracy:  51.2%  test loss:  1.454 test accuracy:  55.0%\n",
      "epoch: 87 training loss:  0.140 training accuracy:  52.5%  test loss:  1.673 test accuracy:  55.0%\n",
      "epoch: 88 training loss:  0.147 training accuracy:  52.5%  test loss:  1.657 test accuracy:  55.0%\n",
      "epoch: 89 training loss:  0.170 training accuracy:  48.8%  test loss:  1.550 test accuracy:  55.0%\n",
      "epoch: 90 training loss:  0.320 training accuracy:  56.2%  test loss:  1.767 test accuracy:  40.0%\n",
      "epoch: 91 training loss:  0.271 training accuracy:  55.0%  test loss:  1.744 test accuracy:  50.0%\n",
      "epoch: 92 training loss:  0.332 training accuracy:  52.5%  test loss:  1.523 test accuracy:  55.0%\n",
      "epoch: 93 training loss:  0.206 training accuracy:  52.5%  test loss:  1.434 test accuracy:  55.0%\n",
      "epoch: 94 training loss:  0.324 training accuracy:  60.0%  test loss:  1.584 test accuracy:  50.0%\n",
      "epoch: 95 training loss:  0.230 training accuracy:  60.0%  test loss:  1.644 test accuracy:  45.0%\n",
      "epoch: 96 training loss:  0.164 training accuracy:  56.2%  test loss:  1.452 test accuracy:  40.0%\n",
      "epoch: 97 training loss:  0.262 training accuracy:  51.2%  test loss:  1.494 test accuracy:  45.0%\n",
      "epoch: 98 training loss:  0.277 training accuracy:  52.5%  test loss:  1.342 test accuracy:  65.0%\n",
      "epoch: 99 training loss:  0.134 training accuracy:  55.0%  test loss:  1.314 test accuracy:  65.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PointNet_2Multimodal_withDNN(\n",
       "  (transform): Transform(\n",
       "    (input_transform): Tnet(\n",
       "      (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
       "      (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "      (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
       "      (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (fc3): Linear(in_features=256, out_features=9, bias=True)\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (feature_transform): Tnet(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "      (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "      (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
       "      (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (fc3): Linear(in_features=256, out_features=4096, bias=True)\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
       "    (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "    (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
       "    (fc1): Linear(in_features=3, out_features=64, bias=True)\n",
       "    (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (fc4): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (fc5): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (transform2): Transform_2Multi(\n",
       "    (input_transform): Tnet(\n",
       "      (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
       "      (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "      (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
       "      (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (fc3): Linear(in_features=256, out_features=9, bias=True)\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (feature_transform): Tnet(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "      (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "      (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
       "      (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (fc3): Linear(in_features=256, out_features=4096, bias=True)\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
       "    (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "    (conv3): Conv1d(128, 512, kernel_size=(1,), stride=(1,))\n",
       "    (fc1): Linear(in_features=3, out_features=64, bias=True)\n",
       "    (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (fc4): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (fc5): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (transform3): DNNModel(\n",
       "    (Flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (fc1): Linear(in_features=172, out_features=512, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (loss_fn): CrossEntropyLoss()\n",
       "    (logsoftmax): LogSoftmax(dim=1)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=640, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (logsoftmax): LogSoftmax(dim=1)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pointnetfunct.PointNet_struct import PointNet_2Multimodal_withDNN\n",
    "from pointnetfunct.PointNet_trainingfunct import run_model_2multi_head_dnn\n",
    "pointnet_2mh = PointNet_2Multimodal_withDNN(classes=2)\n",
    "model_path = \"./final_models/2000points/cut1/pointnet_cut1_2branch_uniform_2000pt.pth\"\n",
    "load_model(pointnet_2mh,model_path)\n",
    "\n",
    "run_model_2multi_head_dnn(train_loader_input = train_loader,\n",
    "                        vaild_loader_input = valid_loader,\n",
    "                        nepochs = 100, \n",
    "                        modelnet = pointnet_2mh,\n",
    "                        results_path = \"./2branch_cut1_uniform_2000pt_transfer\", \n",
    "                        filename = \"/transfer.pt\",\n",
    "                        model_name=\"./2branch_cut1_uniform_2000pt_transfer.pth\")\n",
    "pointnet_2mh.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointnet_2mh.to(\"cpu\")\n",
    "pointnet_copy = PointNet_2Multimodal_withDNN(classes=2)\n",
    "model_path = \"./2branch_cut1_uniform_2000pt_transfer.pth\"\n",
    "#pointnet_copy = PointNet_2Multimodal_withDNN(classes=2)\n",
    "# model_path = \"./final_models/2000points/cut1/pointnet_cut1_2branch_uniform_2000pt.pth\"\n",
    "load_model(pointnet_copy,model_path)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_pred_result = []\n",
    "y_probs = []\n",
    "pointnet_copy = pointnet_copy.to(\"cpu\")\n",
    "with torch.no_grad():\n",
    "    for inputs_v,inputs_v2,inputs_v3,labels in valid_loader:\n",
    "        inputs_v = inputs_v.to(torch.float32)\n",
    "        inputs_v = inputs_v.squeeze(1).permute(0, 2, 1)\n",
    "        inputs_v2 = inputs_v2.to(torch.float32)\n",
    "        inputs_v2 = inputs_v2.squeeze(1).permute(0, 2, 1)\n",
    "        inputs_v3 = inputs_v3.to(torch.float32)\n",
    "        \n",
    "        labels = labels.to(torch.long)\n",
    "        outputs = pointnet_copy.forward(inputs_v,inputs_v3)\n",
    "        \n",
    "        #print(torch.exp(outputs))\n",
    "\n",
    "        outputs_value = (torch.max(torch.exp(outputs), 1)[0]).data.cpu().numpy()\n",
    "        outputs_result = (torch.max(torch.exp(outputs), 1)[1]).data.cpu().numpy()\n",
    "        \n",
    "        \n",
    "        #print(outputs)\n",
    "        y_pred.extend(outputs_value)\n",
    "        y_pred_result.extend(outputs_result)\n",
    "            \n",
    "        labels = labels.data.cpu().numpy()\n",
    "        y_true.extend(labels) \n",
    "        y_probs.extend(outputs)\n",
    "\n",
    "from pointnetfunct.evaluation import show_cm_dl,show_graph,print_values,draw_rocgraph\n",
    "\n",
    "draw_rocgraph(y_pred_result,y_true, y_probs, name=\"2000pt_2branch_AOC\", interval=0.04)\n",
    "show_cm_dl(y_true, y_pred,y_pred_result,name=\"2000pt_2branch_CM\")\n",
    "print_values (y_true, y_pred_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
